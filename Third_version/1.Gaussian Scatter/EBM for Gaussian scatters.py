# -*- coding: utf-8 -*-
"""EBM_LD_sampling_second_version_in_notebook__24_5_2021.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GtI72PmEQDm1JaVu5RNJviQYREZcMLbk

This code refers to the code from URL:https://github.com/swyoon/pytorch-energy-based-model

And the URL: https://github.com/davidlibland/langevin_energy_model/blob/master/energy_model/mcmc/mala.py
"""

import numpy as np
import sklearn.datasets
import time
import random

from matplotlib import pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.optim import Adam
from torch.utils.data import TensorDataset, DataLoader

from torch import autograd
from torch.autograd import Variable

from torch.distributions import MultivariateNormal, Normal, Independent, Uniform

import math

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def setup_seed(seed):
     torch.manual_seed(seed)
     torch.cuda.manual_seed_all(seed)
     np.random.seed(seed)
     random.seed(seed)
     torch.backends.cudnn.deterministic = True

setup_seed(199594)

"""# Energy: Neural network."""

# Fully Connected Network
class FFFNet(nn.Module):
    def __init__(self, in_chan=2, out_chan=2):
        super(FFFNet, self).__init__()
        self.l1 = nn.Linear(in_chan,16)
        self.l2 = nn.Linear(16,64)
        self.l3 = nn.Linear(64,16)
        self.l4 = nn.Linear(16,out_chan)

    def forward(self, x):
        x = self.l1(x)
        x = F.relu(x)
        x = self.l2(x)
        x = F.relu(x)
        x = self.l3(x)
        x = F.relu(x)
        x = self.l4(x)
        return x

"""# Two Langevin method: SGLD&MALA

# SGLD (noise)
"""

def sample_langevin(x, Energy, stepsize, n_steps, noise_scale=None, intermediate_samples=False):
    if noise_scale is None:
        noise_scale = np.sqrt(stepsize * 2)

    l_samples = []
    x.requires_grad = True
    for _ in range(n_steps):
        
        noise = torch.empty_like(x, dtype=torch.float32).normal_(0, 1)*noise_scale

        out = Energy(x)

        grad = autograd.grad(out.sum(), x, only_inputs=True)[0]

        dynamics = -stepsize * grad + noise
        
        x = x + dynamics

        l_samples.append(x.detach().to(device))

    if intermediate_samples:
        return l_samples
    else:
        return l_samples[-1]

"""just for debugging, please do not run this function.

# Gradient Descent(no noise)
For experiment, try sampling without the noise.
"""

def sample_langevin_no_noise(x, Energy, stepsize, n_steps, noise_scale=None, intermediate_samples=False):
    if noise_scale is None:
        noise_scale = np.sqrt(stepsize * 2)

    l_samples = []
    x.requires_grad = True

    for _ in range(n_steps):
        noise = torch.empty_like(x, dtype=torch.float32).normal_(0, 1)*noise_scale

        out = Energy(x)
        
        grad = autograd.grad(out.sum(), x, only_inputs=True)[0]

        dynamics = -stepsize * grad
        x = x + dynamics

        l_samples.append(x.detach().to(device))

    if intermediate_samples:
        return l_samples
    else:
        return l_samples[-1]

"""# MALA
The math method for log_q_xx_ in MALA

This part of code refers to the code from URL: https://github.com/davidlibland/langevin_energy_model/blob/master/energy_model/mcmc/mala.py
"""

def log_q(Energy, lr, x_, x):
    x.requires_grad_(True)
    if x.grad is not None:
        x.grad.data.zero_()
    y = Energy(x).sum()
    y.backward()
    grad_x = x.grad

    eps = ((x_ - x + lr * grad_x) ** 2).sum(dim=1)
    return -eps / (4 * lr)

"""mala implement"""

def sample_MALA(x, Energy, stepsize, n_steps, noise_scale=None, intermediate_samples=False):
      
    l_samples = []
    x.requires_grad = True
    acceptance_ratio_original = 0.5

    for _ in range(n_steps):

        noise_scale = np.sqrt(stepsize * 2)
        x.requires_grad_(True)

        noise = torch.empty_like(x, dtype=torch.float32).normal_(0, 1)*noise_scale

        if x.grad is not None:
            x.grad.data.zero_()

        out = Energy(x)
        #out.sum().backward()
        #grad_1 = x.grad
        
        grad_1 = autograd.grad(out.sum(), x, only_inputs=True)[0]
        #stepsize = stepsize / max(1, float(grad.abs().max()))
        
        dynamics = -stepsize * grad_1 + noise
        x_ = x + dynamics
        
        qx_x = torch.exp(1/(-4 * stepsize)*torch.sum(((x_ - x + stepsize*grad_1) ** 2),dim = 1))

        x_.requires_grad_(True)
        if x_.grad is not None:
            x_.grad.data.zero_()
        out_ = Energy(x_)
        grad_2 = autograd.grad(out_.sum(), x_, only_inputs=True)[0]

        qxx_ = torch.exp(1/(-4 * stepsize)*torch.sum(((x - x_ + stepsize*grad_2) ** 2),dim = 1))

        up = qxx_ * out_.t()
        down = qx_x * out.t()
        length = len(up)
        #print(qxx_)
        ratio = min(1, (up/down).sum(dim = 1)/length)


        if (ratio > acceptance_ratio_original):
          x = x_

        result = x.detach()

        l_samples.append(result.to(device))

    if intermediate_samples:
        return l_samples
    else:
        return l_samples[-1]
        #return l_samples, l_samples[-1]

"""# pSGLD"""

def sample_pSGLD(x, Energy, stepsize, n_steps, noise_scale=None, intermediate_samples=False, beta = 0.99, Lambda = 1e-15):
    if noise_scale is None:
        noise_scale = np.sqrt(stepsize * 2)
    
    V = torch.zeros_like(x)

    l_samples = []
    x.requires_grad = True

    for _ in range(n_steps):
      
      x.requires_grad_(True)
      
      if x.grad is not None:
          x.grad.data.zero_()
      
      out = Energy(x)
      out.sum().backward(retain_graph=True)
      grad = x.grad


      V = torch.addcmul(V.mul_(beta), grad, grad, value=1 - beta)

      G = V.sqrt().add_(Lambda)
      

      x = torch.addcdiv(x, grad, G, value=-stepsize)

      noise_std = 2 * stepsize/ G

      noise_scale = noise_std.sqrt()

      noise = torch.empty_like(x, dtype=torch.float32).normal_(0, 1)*noise_scale
      x = x + noise
      l_samples.append(x.detach().to(device))
      x.retain_grad()

    if intermediate_samples:
        return l_samples
    else:
        return l_samples[-1]

"""# Create data

4 Gaussian distribution.
"""

def sample_2d_data(dataset, n_samples):
    zz = torch.randn(n_samples, 2)
    z = torch.empty_like(zz, dtype=torch.float32).normal_(0, 10)

    if dataset == '4gaussians':
        scale = 5
        sq2 = math.sqrt(2)
        centers = [(2,0), (-2,0), (0,2), (0,-2)]

        centers = torch.tensor([(scale * x, scale * y) for x,y in centers])
        return (0.05 * z + centers[torch.randint(len(centers), size=(n_samples,))])

"""**Show the distribution graph.**"""

def plot_scatter(neg_x, X_train):

  xmin = -30
  xmax = 30
  ymin = -30
  ymax = 30
  plt.style.use('seaborn-darkgrid') 
  plt.figure(figsize=(10, 10))

  plt.xlim(xmin, xmax)
  plt.ylim(ymin, ymax)
  plt.title("Training samples", fontsize=20)
  plt.scatter(X_train[:,:1], X_train[:,1:], alpha=0.5, color='gray', marker='o')
  plt.scatter(neg_x[:,:1], neg_x[:,1:], alpha=0.3, color='green', marker='.')
  plt.show()

def plot_scatter_train(X_train):

  xmin = -30
  xmax = 30
  ymin = -30
  ymax = 30
  plt.style.use('seaborn-darkgrid') 
  plt.figure(figsize=(10, 10))

  plt.xlim(xmin, xmax)
  plt.ylim(ymin, ymax)
  plt.title("Training samples", fontsize=20)
  plt.scatter(X_train[:,:1], X_train[:,1:], alpha=0.5, color='gray', marker='o')
  plt.show()

X_train = sample_2d_data('4gaussians', 500)
neg_x = X_train + 1*torch.empty_like(X_train, dtype=torch.float32).normal_(0, 12) 
plot_scatter_train(X_train)
plot_scatter(neg_x, X_train)

"""Train with Langevin.

The problem now is that Langevin does not sample from the area with high probability. Although from the value of lose, it is successful. But from the scatters graph, the effect is not well.

# Train part

# plot the counter with scatter and trace graph.
"""

import numpy as np
import matplotlib.pyplot as plt

def plot_contter_scatters(Energy, points, X_train):

  
  step = 0.2
  x = np.arange(-30,30,step)
  y = np.arange(-30,30,step)
  
  X,Y = np.meshgrid(x,y)

  xmin = -30
  xmax = 30
  ymin = -30
  ymax = 30
  plt.figure(figsize=(10, 10))

  plt.xlim(xmin, xmax)
  plt.ylim(ymin, ymax)

  points_list = []

  for i in range(300):
    for j in range(300):
      points_list.append([x[i],y[j]])

  points_list=torch.Tensor(points_list)

  energy = Energy(points_list.cpu()).detach().numpy()
  energy_list = []

  for k in range(len(energy)):
    energy_list.append(energy[k][0].tolist())
  
  energy_2d_list = [[energy_list[j*300 + i] for j in range(300)] for i in range(300)]

  cset = plt.contourf(x,y,energy_2d_list,10,cmap=plt.cm.coolwarm)
  contour = plt.contour(x,y,energy_2d_list,1,colors='k')
  
  #plt.scatter(X_train[:,:1], X_train[:,1:], alpha=0.5, color='gray', marker='o')
  points = np.array(points)
  plt.scatter(points[:,:1], points[:,1:], alpha=0.2, color='green', marker='.')
  
  plt.clabel(contour,fontsize=10,colors='k')
  plt.colorbar(cset)
  
  plt.show()

import numpy as np
import matplotlib.pyplot as plt

def plot_trace(Energy, points):

  
  step = 0.2
  x = np.arange(-30,30,step)
  y = np.arange(-30,30,step)
  
  X,Y = np.meshgrid(x,y)

  xmin = -30
  xmax = 30
  ymin = -30
  ymax = 30
  plt.figure(figsize=(10, 10))

  plt.xlim(xmin, xmax)
  plt.ylim(ymin, ymax)

  points_list = []

  for i in range(300):
    for j in range(300):
      points_list.append([x[i],y[j]])

  points_list=torch.Tensor(points_list)

  energy = Energy(points_list.cpu()).detach().numpy()
  energy_list = []

  for k in range(len(energy)):
    energy_list.append(energy[k][0].tolist())
  
  energy_2d_list = [[energy_list[j*300 + i] for j in range(300)] for i in range(300)]

  cset = plt.contourf(x,y,energy_2d_list,15,cmap=plt.cm.coolwarm)
  contour = plt.contour(x,y,energy_2d_list,1,colors='k')

  points = np.array(points)
  plt.plot(points[:,:1], points[:,1:], alpha=0.5, color='white', marker='.')
  plt.scatter(points[0][0],points[0][1], marker= '*', alpha = 1, color = 'green', s=16**2)
  plt.scatter(points[-1][0],points[-1][1],marker= '^', alpha = 1, color = 'green', s=16**2)

  plt.clabel(contour,fontsize=10,colors='k')
  plt.colorbar(cset)
  
  plt.show()

"""# Part 1: Langevin Sampling without noise. (Gradient descent)"""

Energy = FFFNet(in_chan = 2, out_chan = 1)
opt = Adam(Energy.parameters(), lr=1e-3)
#scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.2)

f_loss = nn.L1Loss()
stepsize = 1e-2
n_steps = 200
n_epoch = 600
#train_dl = DataLoader(TensorDataset(X_train), batch_size=16, shuffle=True, num_workers=0)

for epoch in range(n_epoch):
  points=[]
  l_loss = []
  j=0
  
  pos_x = X_train
  
  neg_x = X_train + 1*torch.empty_like(pos_x, dtype=torch.float32).normal_(0, 10)
  neg_x = sample_langevin_no_noise(neg_x, Energy, stepsize, n_steps, intermediate_samples=False)
  point = neg_x.cpu().numpy()
  for k in range(len(point)):
    points.append(point[k].tolist())

  opt.zero_grad()

  pos_out = Energy(pos_x.cpu())
  neg_out = Energy(neg_x.cpu())

  loss = (pos_out - neg_out) + 0.1 * (pos_out ** 2 + neg_out ** 2)
  loss = loss.mean()
    
  loss.backward()

  torch.nn.utils.clip_grad_norm_(Energy.parameters(), max_norm=0.1)
  opt.step()

  l_loss.append(loss.item())

  print(np.mean(l_loss))
  print(epoch)

  plot_contter_scatters(Energy, points, X_train)

stepsize = 1e-2
n_steps = 8000
points = []
pos_x = X_train
  
neg_x = pos_x + 1*torch.empty_like(pos_x, dtype=torch.float32).normal_(0, 10)
neg_x = sample_MALA(neg_x, Energy, stepsize, n_steps, intermediate_samples=False)
point = neg_x.cpu().numpy()
for k in range(len(point)):
  points.append(point[k].tolist())

plot_contter_scatters(Energy, points, X_train)

stepsize = 1e-2
n_steps = 10000
n_epoch = 100

points = []
  
neg_x = torch.tensor([[20.0,20.0]])
neg_x = sample_langevin_no_noise(neg_x, Energy, stepsize, n_steps, intermediate_samples=True)
k=0
while(k < n_steps):
  point = neg_x[k][0].cpu().tolist()
  points.append(point)
  k+=100

plot_trace(Energy, points)

"""# Part 2 : Langevin Sampling with noise"""

Energy = FFFNet(in_chan = 2, out_chan = 1)
opt = Adam(Energy.parameters(), lr=1e-3)
#scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.2)

#f_loss = nn.L1Loss()
stepsize = 1e-2
n_steps = 200
n_epoch = 500

for epoch in range(n_epoch):
  points=[]
  l_loss = []
  j=0
  
  pos_x = X_train
  
  neg_x = pos_x + 1*torch.empty_like(pos_x, dtype=torch.float32).normal_(0, 10)
  neg_x = sample_langevin(neg_x, Energy, stepsize, n_steps, intermediate_samples=False)
  point = neg_x.cpu().numpy()
  for k in range(len(point)):
    points.append(point[k].tolist())

  opt.zero_grad()

  pos_out = Energy(pos_x.cpu())
  neg_out = Energy(neg_x.cpu())

  loss = (pos_out - neg_out) + 0.1 * (pos_out ** 2 + neg_out ** 2)
  loss = loss.mean()
    
  loss.backward()

  torch.nn.utils.clip_grad_norm_(Energy.parameters(), max_norm=0.1)
  opt.step()

  l_loss.append(loss.item())

  print(np.mean(l_loss))
  print(epoch)

  plot_contter_scatters(Energy, points, X_train)

"""# Load"""

torch.save(Energy.state_dict(),'energy.pth')

Energy = FFFNet(in_chan = 2, out_chan = 1)

state_dict=torch.load('energy.pth')
Energy.load_state_dict(state_dict)

stepsize = 1e-2
n_steps = 4000
points = []
pos_x = X_train
  
neg_x = X_train + 1*torch.empty_like(pos_x, dtype=torch.float32).normal_(0, 10)

neg_x = sample_MALA(neg_x, Energy, stepsize, n_steps, intermediate_samples=False)

point = neg_x.cpu().numpy()
for k in range(len(point)):
  points.append(point[k].tolist())

plot_contter_scatters(Energy, points, X_train)

X_train_test = sample_2d_data('8gaussians', 1000)

"""# R-hat"""

def within_singal_chain_average(x, n_steps, burn_in = 0):
  #calculate the average value of a singal chain.
  sum_in_singal_chain = 0
  for i in range(n_steps - burn_in):
    sum_in_singal_chain += x[i+burn_in]
  ave_in_singal_chain = sum_in_singal_chain/(n_steps-burn_in)
  return ave_in_singal_chain

def within_singal_chain_square(x, ave_in_singal_chain, n_steps, burn_in = 0):
  #calculate the average square of a singal chain 
  sum_square = 0
  for i in range(n_steps - burn_in):
    sum_square += (x[i+burn_in] - ave_in_singal_chain) ** 2
  ave_square = sum_square/(n_steps-burn_in-1)
  return ave_square

def W_whitin_chain(all_chains_sampling, n_steps, burn_in, num_chains):
  average_each_singal_chain = []
  for i in range(num_chains):
    ave_in_singal_chain = within_singal_chain_average(all_chains_sampling[i],n_steps,burn_in)
    average_each_singal_chain.append(ave_in_singal_chain)
  
  square_each_singal_chain = []
  for i in range(num_chains):
    ave_square = within_singal_chain_square(all_chains_sampling[i], average_each_singal_chain[i],n_steps, burn_in)
    square_each_singal_chain.append(ave_square)

  square_sum_all_chain = 0
  for i in range(num_chains):
    square_sum_all_chain += square_each_singal_chain[i]
  
  square_average_all_chain = square_sum_all_chain / num_chains 

  return square_average_all_chain

def B_between_chain(all_chains_sampling, n_steps, burn_in, num_chains):

  average_each_singal_chain = []
  sum_all_chain = 0
  for i in range(num_chains):
    ave_in_singal_chain = within_singal_chain_average(all_chains_sampling[i],n_steps,burn_in)
    average_each_singal_chain.append(ave_in_singal_chain)
    sum_all_chain += average_each_singal_chain[i]
  
  average_all_chain = sum_all_chain / num_chains

  sum_square_between_chain = 0
  for i in range(num_chains):
    sum_square_between_chain += (average_each_singal_chain[i] - average_all_chain) ** 2
  
  B = sum_square_between_chain * (n_steps-burn_in) / (num_chains - 1)
  
  return B

num_chains = 4
all_chains_initial = []

for i in range(num_chains):
  neg_x = X_train + 1*torch.empty_like(X_train, dtype=torch.float32).normal_(0, 2)
  all_chains_initial.append(neg_x)
  
n_steps = 5000
stepsize = 1e-2
all_chains_sampling = []
burn_in = 4000

for i in range(num_chains):
  neg_x_sampling = sample_MALA(all_chains_initial[i], Energy, stepsize, n_steps, intermediate_samples=True)
  out_i = []
  for j in range(n_steps):
    out = Energy(neg_x_sampling[j].cpu())
    out_i.append(out)
  all_chains_sampling.append(out_i)


W = W_whitin_chain(all_chains_sampling, n_steps, burn_in, num_chains)
B = B_between_chain(all_chains_sampling, n_steps, burn_in, num_chains)

VAR = (((num_chains-1) / num_chains) * W + (1 / num_chains) * B)

R = torch.sqrt(VAR/W)
R

ave = sum(R,0)
ave /= 1000
ave

neg_x1 = X_train + 1*torch.empty_like(pos_x, dtype=torch.float32).normal_(0, 15)

stepsize = 1e-2
n_steps = 10000

points = []

#neg_x = 1*torch.empty_like(pos_x, dtype=torch.float32).normal_(0, 10)
neg_x = torch.tensor([[15.0,15.0]])
neg_x = sample_langevin(neg_x, Energy, stepsize, n_steps, intermediate_samples=True)
k=0
while(k < n_steps):
  point = neg_x[k][0].cpu().tolist()
  points.append(point)
  k+=200

plot_trace(Energy, points)

"""# Part 3 : MALA Sampling"""

Energy = FFFNet(in_chan = 2, out_chan = 1)
opt = Adam(Energy.parameters(), lr=1e-3)
#scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.2)

f_loss = nn.L1Loss()
stepsize = 1e-2
n_steps = 200
n_epoch = 500
#train_dl = DataLoader(TensorDataset(X_train), batch_size=16, shuffle=True, num_workers=0)

for epoch in range(n_epoch):
  points=[]
  l_loss = []
  j=0
  
  pos_x = X_train
  
  neg_x = pos_x + 1*torch.empty_like(pos_x, dtype=torch.float32).normal_(0, 10)
  neg_x = sample_MALA(neg_x, Energy, stepsize, n_steps, intermediate_samples=False)
  point = neg_x.cpu().numpy()
  for k in range(len(point)):
    points.append(point[k].tolist())

  opt.zero_grad()

  pos_out = Energy(pos_x.cpu())
  neg_out = Energy(neg_x.cpu())

  loss = (pos_out - neg_out) + 0.1 * (pos_out ** 2 + neg_out ** 2)
  loss = loss.mean()
    
  loss.backward()

  torch.nn.utils.clip_grad_norm_(Energy.parameters(), max_norm=0.1)
  opt.step()

  l_loss.append(loss.item())

  print(np.mean(l_loss))
  print(epoch)

  plot_contter_scatters(Energy, points, X_train)

stepsize = 1e-2
n_steps = 5000

points = []
pos_x = X_train
  
neg_x = X_train + 1*torch.empty_like(pos_x, dtype=torch.float32).normal_(0, 10)
neg_x = sample_MALA(neg_x, Energy, stepsize, n_steps, intermediate_samples=False)
point = neg_x.cpu().numpy()
for k in range(len(point)):
  points.append(point[k].tolist())

plot_contter_scatters(Energy, points, X_train)

stepsize = 1e-2
n_steps = 10000
n_epoch = 100

points = []

#neg_x = 1*torch.empty_like(pos_x, dtype=torch.float32).normal_(0, 10)
neg_x = torch.tensor([[15.0,15.0]])
neg_x = sample_langevin(neg_x, Energy, stepsize, n_steps, intermediate_samples=True)
k=0
while(k < n_steps):
  point = neg_x[k][0].cpu().tolist()
  points.append(point)
  k+=100

plot_trace(Energy, points)

"""# Part 4. PSGLD"""

Energy = FFFNet(in_chan = 2, out_chan = 1)
opt = Adam(Energy.parameters(), lr=1e-3)
#scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.2)

f_loss = nn.L1Loss()
stepsize = 1e-2
n_steps = 200
n_epoch = 600
#train_dl = DataLoader(TensorDataset(X_train), batch_size=16, shuffle=True, num_workers=0)

for epoch in range(n_epoch):
  points=[]
  l_loss = []
  j=0
  
  pos_x = X_train
  
  neg_x = 1*torch.empty_like(pos_x, dtype=torch.float32).normal_(0, 10)
  neg_x = sample_pSGLD(neg_x, Energy, stepsize, n_steps, intermediate_samples=False)
  point = neg_x.cpu().numpy()
  for k in range(len(point)):
    points.append(point[k].tolist())

  opt.zero_grad()

  pos_out = Energy(pos_x.cpu())
  neg_out = Energy(neg_x.cpu())

  loss = (pos_out - neg_out) + 0.1 * (pos_out ** 2 + neg_out ** 2)
  loss = loss.mean()
    
  loss.backward()

  torch.nn.utils.clip_grad_norm_(Energy.parameters(), max_norm=0.1)
  opt.step()

  l_loss.append(loss.item())

  print(np.mean(l_loss))
  print(epoch)

  plot_contter_scatters(Energy, points, X_train)

stepsize = 1e-2
n_steps = 1000
points = []
pos_x = X_train
  
neg_x = 1*torch.empty_like(pos_x, dtype=torch.float32).normal_(0, 10)
neg_x = sample_pSGLD(neg_x, Energy, stepsize, n_steps, intermediate_samples=False)
point = neg_x.cpu().numpy()
for k in range(len(point)):
  points.append(point[k].tolist())

plot_contter_scatters(Energy, points, X_train)

stepsize = 1e-2
n_steps = 10000
n_epoch = 100

points = []


#neg_x = 1*torch.empty_like(pos_x, dtype=torch.float32).normal_(0, 10)
neg_x = torch.tensor([[15.0,15.0]])
neg_x = sample_langevin(neg_x, Energy, stepsize, n_steps, intermediate_samples=True)
k=0
while(k < n_steps):
  point = neg_x[k][0].cpu().tolist()
  points.append(point)
  k+=100

plot_trace(Energy, points)